{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "import lasagne\n",
    "import lasagne.nonlinearities as nonlin\n",
    "import pylab\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "import imageio\n",
    "import time\n",
    "from IPython.display import display, clear_output\n",
    "import PIL\n",
    "\n",
    "from net.pretrained.vgg16.file_worker import make_network\n",
    "from other.preprocess_img import preprocess, resize_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir='data/annotations_trainval2017'\n",
    "dataType='val2017'\n",
    "annFile='{}/annotations/captions_{}.json'.format(dataDir, dataType)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# img2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imgs_load(dir_name):\n",
    "    imgs = {}\n",
    "    for file_name in os.listdir(dir_name + '/data'):\n",
    "        if '.' in file_name and file_name.split('.')[1] == 'data':\n",
    "            with open('{}/data/{}'.format(dir_name, file_name), 'rb') as fl:\n",
    "                data = pickle.load(fl)\n",
    "                imgs.update(data)\n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4991"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imgs = imgs_load(dataDir)\n",
    "display(len(imgs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from other.sents_process import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "danno = sents_load(dataDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-ab1a1959799b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdanno\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manno\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdanno\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdanno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "danno = {}\n",
    "for id, v in anno:\n",
    "    danno[id] = danno.get(id, []) + [v]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from net.train import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "H = 300\n",
    "W = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23968\n"
     ]
    }
   ],
   "source": [
    "train_size = 23300\n",
    "\n",
    "X, y = [], []\n",
    "for id, data in imgs.items():\n",
    "    id = id.lstrip('0')\n",
    "    try:\n",
    "        for ls in danno[id]:\n",
    "            vects = []\n",
    "            for elem in ls:\n",
    "                if not isinstance(elem, str):\n",
    "                    vects.append(elem)\n",
    "            vects = np.pad(vects, [(0, W), (0, 0)], mode='constant', constant_values=0)[:W].T\n",
    "            X.append([vects])\n",
    "            y.append(data[1])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(len(X))\n",
    "\n",
    "X_train, y_train, X_val, y_val = X[:train_size], y[:train_size], X[train_size:], y[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# new version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_cnn(file_name=None):\n",
    "    net = NeuralNet(must_have=[\n",
    "            'input_shape', \n",
    "            'learning_rate', \n",
    "            'train_fun', \n",
    "            'loss_fun', \n",
    "            'loss_fun_det', \n",
    "            'predict_fun_det'])\n",
    "    \n",
    "    input_shape = [None, H, W]\n",
    "    \n",
    "    input_X = T.tensor3(\"input X\", dtype='float32')\n",
    "    target_y = T.matrix(\"target Y\", dtype='float32')\n",
    "\n",
    "    \n",
    "    net['inp'] = lasagne.layers.InputLayer(input_shape, input_var=input_X)\n",
    "    \n",
    "    net['max'] = lasagne.layers.GlobalPoolLayer(net['inp'], pool_function=theano.tensor.max)\n",
    "    net['min'] = lasagne.layers.GlobalPoolLayer(net['inp'], pool_function=theano.tensor.min)\n",
    "    net['mean'] = lasagne.layers.GlobalPoolLayer(net['inp'], pool_function=theano.tensor.mean)\n",
    "    \n",
    "    net['con_2'] = lasagne.layers.Conv1DLayer(net['inp'], num_filters=64, filter_size=2,nonlinearity=None)\n",
    "    net['con_3'] = lasagne.layers.Conv1DLayer(net['inp'], num_filters=64, filter_size=3,nonlinearity=None)\n",
    "    net['con_4'] = lasagne.layers.Conv1DLayer(net['inp'], num_filters=64, filter_size=4,nonlinearity=None)\n",
    "    \n",
    "    boltzmann_max = lambda a, axis: T.sum(a * T.exp(a), axis=-1) / T.exp(a).sum(-1)\n",
    "    \n",
    "    net['gmax_2b'] = lasagne.layers.GlobalPoolLayer(net['con_2'], pool_function=boltzmann_max)\n",
    "    net['gmax_3b'] = lasagne.layers.GlobalPoolLayer(net['con_3'], pool_function=boltzmann_max)\n",
    "    net['gmax_4b'] = lasagne.layers.GlobalPoolLayer(net['con_4'], pool_function=boltzmann_max)\n",
    "    \n",
    "    net['merge'] = lasagne.layers.ConcatLayer((net['max'], net['min'], net['mean'], \n",
    "                                               net['gmax_2b'], net['gmax_3b'], net['gmax_4b']))\n",
    "    \n",
    "    net['batch_0'] =  lasagne.layers.batch_norm(net['merge'])\n",
    "    \n",
    "    net['dens_1'] = lasagne.layers.DenseLayer(net['batch_0'], num_units=1000, nonlinearity=nonlin.elu)\n",
    "    net['batch_1'] =  lasagne.layers.batch_norm(net['dens_1'])\n",
    "    net['drop_1'] = lasagne.layers.DropoutLayer(net['batch_1'], p=0.6)\n",
    "    \n",
    "    net['dens_2'] = lasagne.layers.DenseLayer(net['drop_1'], num_units=1000, nonlinearity=nonlin.elu)\n",
    "    net['batch_2'] =  lasagne.layers.batch_norm(net['dens_2'])\n",
    "    net['drop_2'] = lasagne.layers.DropoutLayer(net['batch_2'], p=0.6)\n",
    "    \n",
    "    net['last'] = lasagne.layers.DenseLayer(net['drop_2'], num_units=4096)\n",
    "    \n",
    "    \n",
    "    y_predicted = lasagne.layers.get_output(net['last'])\n",
    "    y_predicted_det = lasagne.layers.get_output(net['last'], deterministic=True)\n",
    "\n",
    "    all_weights = lasagne.layers.get_all_params(net['last'], trainable=True)\n",
    "    \n",
    "    learning_rate = theano.shared(lasagne.utils.floatX(0.001))\n",
    "    loss = lasagne.objectives.squared_error(target_y, y_predicted).mean()\n",
    "    loss_det = lasagne.objectives.squared_error(target_y, y_predicted_det).mean()\n",
    "    \n",
    "    # loss = loss + lasagne.regularization.regularize_layer_params(net['last'], lasagne.regularization.l2) * 0.01\n",
    "    updates = lasagne.updates.adam(loss, all_weights, learning_rate=learning_rate)\n",
    "    \n",
    "    train_fun = theano.function([input_X, target_y], loss, updates=updates, allow_input_downcast=True)\n",
    "    loss_fun = theano.function([input_X, target_y], loss, allow_input_downcast=True)\n",
    "    loss_fun_det = theano.function([input_X, target_y], loss_det, allow_input_downcast=True)\n",
    "    predict_fun_det = theano.function([input_X], y_predicted_det, allow_input_downcast=True)\n",
    "    \n",
    "    if file_name:\n",
    "        load_net(net['last'], file_name, dataDir)\n",
    "        \n",
    "    def proc_vec(v):\n",
    "        res = []\n",
    "        for vv in v:\n",
    "            res.append(vv[0])\n",
    "        return res\n",
    "    \n",
    "    def proc_1(fun):\n",
    "        def temp(v):\n",
    "            v = proc_vec(v)\n",
    "            return fun(v)\n",
    "        return temp\n",
    "\n",
    "    def proc_2(fun):\n",
    "        def temp(v, u):\n",
    "            v = proc_vec(v)\n",
    "            return fun(v, u)\n",
    "\n",
    "        return temp\n",
    "\n",
    "    net.input_shape = input_shape\n",
    "    net.learning_rate = learning_rate\n",
    "    net.train_fun = proc_2(train_fun)\n",
    "    net.loss_fun = proc_2(loss_fun)\n",
    "    net.loss_fun_det = proc_2(loss_fun_det)\n",
    "    net.predict_fun_det = proc_1(predict_fun_det)\n",
    "    \n",
    "    return net.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NeuralNet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-4cfb4646c5ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-3abc2b74a09a>\u001b[0m in \u001b[0;36mbuild_cnn\u001b[0;34m(file_name)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     net = NeuralNet(must_have=[\n\u001b[0m\u001b[1;32m      3\u001b[0m             \u001b[0;34m'input_shape'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0;34m'learning_rate'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0;34m'train_fun'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'NeuralNet' is not defined"
     ]
    }
   ],
   "source": [
    "net = build_cnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gmax_3b (None, 64) []\n",
      "dens_2 (None, 1000) [(1000, 1000)]\n",
      "con_3 (None, 64, 12) [(64, 300, 3), (64,)]\n",
      "inp (None, 300, 14) []\n",
      "gmax_2b (None, 64) []\n",
      "gmax_4b (None, 64) []\n",
      "con_4 (None, 64, 11) [(64, 300, 4), (64,)]\n",
      "batch_1 (None, 1000) []\n",
      "drop_1 (None, 1000) []\n",
      "merge (None, 1092) []\n",
      "min (None, 300) []\n",
      "batch_0 (None, 1092) [(1092,), (1092,), (1092,), (1092,)]\n",
      "mean (None, 300) []\n",
      "last (None, 4096) [(1000, 4096), (4096,)]\n",
      "dens_1 (None, 1000) [(1092, 1000)]\n",
      "con_2 (None, 64, 13) [(64, 300, 2), (64,)]\n",
      "max (None, 300) []\n",
      "batch_2 (None, 1000) []\n",
      "drop_2 (None, 1000) []\n"
     ]
    }
   ],
   "source": [
    "for key,val in net.items():\n",
    "    if isinstance(val,lasagne.layers.Layer):\n",
    "        params = val.get_params()\n",
    "        print (key, val.output_shape, [p.get_value().shape for p in params])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_c = Training(net, 40)\n",
    "training_c.set_Xy(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import ewma\n",
    "\n",
    "def my_plot(losses_val, losses_train):\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    \n",
    "    losses_train = ewma(np.array(losses_train), span=70)\n",
    "    losses_val = ewma(np.array(losses_val), span=1.2)\n",
    "    \n",
    "    plt.subplot(221)\n",
    "    plt.plot(np.arange(len(losses_val)) * Training.mod, losses_val, 'r', losses_train, 'b')\n",
    "    plt.grid()\n",
    "    plt.legend(['y = loss valid', 'y = loss train'], loc='upper right')\n",
    "\n",
    "    plt.subplot(222)\n",
    "    plt.plot(losses_train[-120:])\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(223)\n",
    "    plt.plot(losses_val)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(224)\n",
    "    plt.plot(losses_val[-60:])\n",
    "    plt.grid()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "training_c.set_ploter(my_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "training_c.training(dataDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_net(net['last'], 'new_cnn2.3v_1.net', dataDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.learning_rate.get_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.learning_rate.set_value(0.0006)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
