{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from pycocotools.coco import COCO\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "import lasagne\n",
    "import lasagne.nonlinearities as nonlin\n",
    "import pylab\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "import imageio\n",
    "import time\n",
    "from IPython.display import display, clear_output\n",
    "import PIL\n",
    "\n",
    "from net.vgg16 import make_network_from_file\n",
    "from net.process import preprocess, resize_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.11s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "dataDir='annotations_trainval2017'\n",
    "dataType='val2017'\n",
    "annFile='{}/annotations/captions_{}.json'.format(dataDir, dataType)\n",
    "\n",
    "coco=COCO(annFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25014\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(541200,\n",
       "  {'caption': 'a glass vase with some flower coming out of it ',\n",
       "   'id': 541200,\n",
       "   'image_id': 159282}),\n",
       " (262135,\n",
       "  {'caption': 'A tennis player is taking a swing on a red court.',\n",
       "   'id': 262135,\n",
       "   'image_id': 551804}),\n",
       " (589821,\n",
       "  {'caption': 'there is a yellow notebook on a black desk ',\n",
       "   'id': 589821,\n",
       "   'image_id': 176446})]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(len(coco.anns))\n",
    "display(list(coco.anns.items())[-3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and load VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# net, prob_and_vec = make_network_from_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# img2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from net.imgs import *\n",
    "      \n",
    "# %time imgs_process('annotations_trainval2017/img') # Wall time: 4h 43min 38s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4991"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imgs = imgs_load(dataDir)\n",
    "display(len(imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max = 12.5911690933\n",
      "min = 0.0\n"
     ]
    }
   ],
   "source": [
    "temp = pickle.load(open('annotations_trainval2017\\data\\\\130.data', 'rb'))\n",
    "temp = list(temp.items())\n",
    "print('max =', np.max(temp[10][1][1]))\n",
    "print('min =', np.min(temp[10][1][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from net.sents_process import *\n",
    "\n",
    "# sents_process(dict_from_anns(coco.anns), 'annotations_trainval2017')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "anno = sent_load(dataDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "danno = {}\n",
    "for id, v in anno:\n",
    "    danno[id] = danno.get(id, []) + [v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Two mean are playing tennis and both are wearing sunglasses.  ',\n",
       " 'a couple of people that are playing in a field',\n",
       " 'A purple and white bus in a parking lot.',\n",
       " 'a tennis player swinging a racket at a ball',\n",
       " 'a person attempting a jump with a skateboard']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "captions = [text[1]['caption'] for text in coco.anns.items()]\n",
    "display(captions[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mix_text_vec2vec(sent):\n",
    "    only_vec = list(filter(lambda x: not isinstance(x, str), sent))\n",
    "    matr = np.vstack(tuple(only_vec))\n",
    "    return np.concatenate((\n",
    "            np.mean(matr, axis=0), \n",
    "            np.max(matr, axis=0), \n",
    "            np.min(matr, axis=0)))\n",
    "\n",
    "vects = {}\n",
    "for id, val in anno:\n",
    "    vects[id] = vects.get(id, []) + [mix_text_vec2vec(val)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from net.train import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "H = 300\n",
    "W = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23968\n"
     ]
    }
   ],
   "source": [
    "train_size = 23300\n",
    "\n",
    "X, y = [], []\n",
    "for id, data in imgs.items():\n",
    "    while id[0] == '0':\n",
    "        id = id[1:]\n",
    "    try:\n",
    "        for ls in danno[id]:\n",
    "            vects = []\n",
    "            for elem in ls:\n",
    "                if not isinstance(elem, str):\n",
    "                    vects.append(elem)\n",
    "            vects = np.pad(vects, [(0, W), (0, 0)], mode='constant', constant_values=0)[:W].T\n",
    "            X.append([vects])\n",
    "            y.append(data[1])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(len(X))\n",
    "\n",
    "X_train, y_train, X_val, y_val = X[:train_size], y[:train_size], X[train_size:], y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#with open('data.net', 'wb') as fl:\n",
    "#    pickle.dump((X_train, y_train, X_val, y_val), fl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# new version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_cnn(file_name=None):\n",
    "    net = NeuralNet(must_have=[\n",
    "            'input_shape', \n",
    "            'learning_rate', \n",
    "            'train_fun', \n",
    "            'loss_fun', \n",
    "            'loss_fun_det', \n",
    "            'predict_fun_det'])\n",
    "    \n",
    "    input_shape = [None, H, W]\n",
    "    \n",
    "    input_X = T.tensor3(\"input X\", dtype='float32')\n",
    "    target_y = T.matrix(\"target Y\", dtype='float32')\n",
    "\n",
    "    \n",
    "    net['inp'] = lasagne.layers.InputLayer(input_shape, input_var=input_X)\n",
    "    \n",
    "    net['max'] = lasagne.layers.GlobalPoolLayer(net['inp'], pool_function=theano.tensor.max)\n",
    "    net['min'] = lasagne.layers.GlobalPoolLayer(net['inp'], pool_function=theano.tensor.min)\n",
    "    net['mean'] = lasagne.layers.GlobalPoolLayer(net['inp'], pool_function=theano.tensor.mean)\n",
    "    \n",
    "    net['con_2'] = lasagne.layers.Conv1DLayer(net['inp'], num_filters=64, filter_size=2,nonlinearity=None)\n",
    "    net['con_3'] = lasagne.layers.Conv1DLayer(net['inp'], num_filters=64, filter_size=3,nonlinearity=None)\n",
    "    net['con_4'] = lasagne.layers.Conv1DLayer(net['inp'], num_filters=64, filter_size=4,nonlinearity=None)\n",
    "    \n",
    "    net['gmax_2'] = lasagne.layers.GlobalPoolLayer(net['con_2'], pool_function=T.max)\n",
    "    net['gmax_3'] = lasagne.layers.GlobalPoolLayer(net['con_3'], pool_function=T.max)\n",
    "    net['gmax_4'] = lasagne.layers.GlobalPoolLayer(net['con_4'], pool_function=T.max)\n",
    "    \n",
    "    net['merge'] = lasagne.layers.ConcatLayer((net['max'], net['min'], net['mean'], \n",
    "                                               net['gmax_2'], net['gmax_3'], net['gmax_4']))\n",
    "    \n",
    "    net['dens_1'] = lasagne.layers.DenseLayer(net['merge'], num_units=500, nonlinearity=nonlin.elu)\n",
    "    net['drop_1'] = lasagne.layers.DropoutLayer(net['dens_1'], p=0.5)\n",
    "    \n",
    "    net['dens_2'] = lasagne.layers.DenseLayer(net['drop_1'], num_units=500, nonlinearity=nonlin.elu)\n",
    "    net['drop_2'] = lasagne.layers.DropoutLayer(net['dens_2'], p=0.5)\n",
    "    \n",
    "    net['last'] = lasagne.layers.DenseLayer(net['drop_2'], num_units=4096)\n",
    "    \n",
    "    \n",
    "    y_predicted = lasagne.layers.get_output(net['last'])\n",
    "    y_predicted_det = lasagne.layers.get_output(net['last'], deterministic=True)\n",
    "\n",
    "    all_weights = lasagne.layers.get_all_params(net['last'], trainable=True)\n",
    "    \n",
    "    learning_rate = theano.shared(lasagne.utils.floatX(0.001))\n",
    "    loss = lasagne.objectives.squared_error(target_y, y_predicted).mean()\n",
    "    loss_det = lasagne.objectives.squared_error(target_y, y_predicted_det).mean()\n",
    "    \n",
    "    # loss = loss + lasagne.regularization.regularize_layer_params(net['last'], lasagne.regularization.l2) * 0.01\n",
    "    updates = lasagne.updates.adam(loss, all_weights, learning_rate=learning_rate)\n",
    "    \n",
    "    train_fun = theano.function([input_X, target_y], loss, updates=updates, allow_input_downcast=True)\n",
    "    loss_fun = theano.function([input_X, target_y], loss, allow_input_downcast=True)\n",
    "    loss_fun_det = theano.function([input_X, target_y], loss_det, allow_input_downcast=True)\n",
    "    predict_fun_det = theano.function([input_X], y_predicted_det, allow_input_downcast=True)\n",
    "    \n",
    "    if file_name:\n",
    "        load_net(net['last'], file_name, dataDir)\n",
    "        \n",
    "    def proc_vec(v):\n",
    "        res = []\n",
    "        for vv in v:\n",
    "            res.append(vv[0])\n",
    "        return res\n",
    "    \n",
    "    def proc_1(fun):\n",
    "        def temp(v):\n",
    "            v = proc_vec(v)\n",
    "            return fun(v)\n",
    "        return temp\n",
    "\n",
    "    def proc_2(fun):\n",
    "        def temp(v, u):\n",
    "            v = proc_vec(v)\n",
    "            return fun(v, u)\n",
    "\n",
    "        return temp\n",
    "\n",
    "    net.input_shape = input_shape\n",
    "    net.learning_rate = learning_rate\n",
    "    net.train_fun = proc_2(train_fun)\n",
    "    net.loss_fun = proc_2(loss_fun)\n",
    "    net.loss_fun_det = proc_2(loss_fun_det)\n",
    "    net.predict_fun_det = proc_1(predict_fun_det)\n",
    "    \n",
    "    return net.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net = build_cnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gmax_3 (None, 64) []\n",
      "last (None, 4096) [(500, 4096), (4096,)]\n",
      "dens_2 (None, 500) [(500, 500), (500,)]\n",
      "inp (None, 300, 14) []\n",
      "mean (None, 300) []\n",
      "con_3 (None, 64, 12) [(64, 300, 3), (64,)]\n",
      "max (None, 300) []\n",
      "drop_2 (None, 500) []\n",
      "dens_1 (None, 500) [(1092, 500), (500,)]\n",
      "gmax_2 (None, 64) []\n",
      "con_4 (None, 64, 11) [(64, 300, 4), (64,)]\n",
      "gmax_4 (None, 64) []\n",
      "min (None, 300) []\n",
      "con_2 (None, 64, 13) [(64, 300, 2), (64,)]\n",
      "drop_1 (None, 500) []\n",
      "merge (None, 1092) []\n"
     ]
    }
   ],
   "source": [
    "for key,val in net.items():\n",
    "    if isinstance(val,lasagne.layers.Layer):\n",
    "        params = val.get_params()\n",
    "        \n",
    "\n",
    "        print (key, val.output_shape, [p.get_value().shape for p in params])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_c = Training(net, 40)\n",
    "training_c.set_Xy(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import ewma\n",
    "\n",
    "def my_plot(losses_val, losses_train):\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    \n",
    "    losses_train = ewma(np.array(losses_train), span=70)\n",
    "    losses_val = ewma(np.array(losses_val), span=1.2)\n",
    "    \n",
    "    plt.subplot(221)\n",
    "    plt.plot(np.arange(len(losses_val)) * Training.mod, losses_val, 'r', losses_train, 'b')\n",
    "    plt.grid()\n",
    "    plt.legend(['y = loss valid', 'y = loss train'], loc='upper right')\n",
    "\n",
    "    plt.subplot(222)\n",
    "    plt.plot(losses_train[-120:])\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(223)\n",
    "    plt.plot(losses_val)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(224)\n",
    "    plt.plot(losses_val[-60:])\n",
    "    plt.grid()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "training_c.set_ploter(my_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_c.training(dataDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_net(net['last'], 'new_cnn2_0.net')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
